{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828a3ac8",
   "metadata": {},
   "source": [
    "### Importing needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d310f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de71ccb",
   "metadata": {},
   "source": [
    "### Loading and saving images as matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5d1681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_NN = [] # dataset array\n",
    "\n",
    "cwd = os.getcwd() + \"\\images\"+ \"\\ \"[0]\n",
    "pokemon_labels = []\n",
    "\n",
    "with os.scandir(cwd) as itr:\n",
    "    i=0\n",
    "    for dir in os.scandir(cwd):\n",
    "        cwd_poke = cwd + dir.name + \"\\ \"[0]\n",
    "        #y = np.zeros((len(os.listdir(cwd)), 1)) # creates the label for each pokemon\n",
    "\n",
    "        with os.scandir(cwd_poke) as itr2 : # iterates every image in each folder and appends name to label list\n",
    "            # these directory lists are sorted but it doesn't matter because the labels are the same (atleast if using VSCode)\n",
    "\n",
    "            for img in itr2:\n",
    "                pokemon_labels.append(i) # appends label number to label list\n",
    "                img_open = open(cwd_poke+img.name, 'rb') # saving image to variable in binary\n",
    "                read_img = img_open.read()\n",
    "\n",
    "                if img.name.endswith(\".jpg\") or img.name.endswith(\".jpeg\"):\n",
    "                    img_decode = tf.image.decode_jpeg(read_img, channels = 3)# /255 # decode jpeg image to matrix of matrices and normalizes (divide by 255) \n",
    "                    resized_img = tf.image.resize_with_pad(img_decode, 150,150) # resizes image \n",
    "                \n",
    "                elif img.name.endswith(\".png\"):\n",
    "                    img_decode = tf.image.decode_png(read_img, channels = 3) #/255 # decode png image to matrix of matrices and normalizes (divide by 255) \n",
    "                    resized_img = tf.image.resize_with_pad(img_decode, 150,150)\n",
    "\n",
    "                data_NN.append(resized_img) # appends image to matrix with images\n",
    "        i+=1\n",
    "\n",
    "data_NN = np.array(data_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162b59c",
   "metadata": {},
   "source": [
    "#### Confirming that the data was processed correctly\n",
    "\n",
    "Transforming an image matrix back to .jpeg format and verifying that it was correctly saved\n",
    "It will be confirmed that the first image in data_NN list is not the first image in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65c3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_NN[0]\n",
    "\n",
    "import PIL\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)\n",
    "\n",
    "tensor_to_image(img).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc997ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6837, 67500)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = data_NN.shape[0]\n",
    "n1 = data_NN.shape[1]\n",
    "n2 = data_NN.shape[2]\n",
    "n3 = data_NN.shape[3]\n",
    "\n",
    "data_final = data_NN.flatten().reshape((m,n1*n2*n3))\n",
    "data_final.shape # transform matrix into working order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e14314",
   "metadata": {},
   "source": [
    "### Computing the Cost Function\n",
    "\n",
    "Recall that the regularized cost function in logistic regression is:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [ -y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "The cost gradients are (remember that the gradient of $\\theta_0$ is not regularized): \n",
    "\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ for $j=0$\n",
    "\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j$ for $j\\geq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87abbd38",
   "metadata": {},
   "source": [
    "#### Defining the sigmoid and regularized cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "920544c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2536948166.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [78], line 32\u001b[1;36m\u001b[0m\n\u001b[1;33m    grad = (1/m) * np.dot(X.transpose(),(h - y))[1:]) + (Lambda/m)* theta[1:]\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Returns sigmoid of z.\n",
    "    \"\"\"\n",
    "    gz = 1/ (1 + np.exp(-z))\n",
    "    return gz\n",
    "\n",
    "def costFunctionReg(X, y, theta, Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array of  data X, labels y and theta, to return the regularized cost function and gradients\n",
    "    of the logistic regression classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples \n",
    "    m = X.shape[1]\n",
    "\n",
    "    #vector of the model predictions for all training examples      \n",
    "    h = sigmoid(np.dot(X,theta)) # forward propagation\n",
    "\n",
    "    error = (-y * np.log(h)) - ((1-y)*np.log(1-h))\n",
    "    \n",
    "    #cost function without regularization term\n",
    "    cost = sum(error)/m\n",
    "    \n",
    "    #add regularization term to the cost function L2 norm\n",
    "    regCost= cost + Lambda/(2*m) * sum(theta[1:]**2)\n",
    "    \n",
    "    #gradient of theta_0\n",
    "    grad_0= (1/m) * np.sum(np.dot(X.transpose(),(h - y))[0])\n",
    "\n",
    "    # #vector of gradients of theta_j from j=1:n (adding the regularization term of the gradient)\n",
    "    grad = (1/m) * np.dot(X.transpose(),(h - y))[1:]) + (Lambda/m)* theta[1:]\n",
    "\n",
    "    # all gradients in a column vector shape\n",
    "    grad_all=np.append(grad_0,grad)\n",
    "    grad_all = grad_all.reshape((len(grad_all), 1))\n",
    "\n",
    "\n",
    "    return regCost[0], grad_all\n",
    "# def costFunctionReg(theta, X, y ,Lambda):\n",
    "#     \"\"\"\n",
    "#     Take in numpy array of theta, X, and y to return the regularize cost function and gradient\n",
    "#     of a logistic regression\n",
    "#     \"\"\"\n",
    "    \n",
    "#     m=len(y)\n",
    "#     y=y[:,np.newaxis]\n",
    "#     predictions = sigmoid(X @ theta)\n",
    "#     error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))\n",
    "#     cost = 1/m * sum(error)\n",
    "#     regCost= cost + Lambda/(2*m) * sum(theta**2)\n",
    "    \n",
    "#     # compute gradient\n",
    "#     j_0= 1/m * (X.transpose() @ (predictions - y))[0]\n",
    "#     j_1 = 1/m * (X.transpose() @ (predictions - y))[1:] + (Lambda/m)* theta[1:]\n",
    "#     grad= np.vstack((j_0[:,np.newaxis],j_1))\n",
    "#     return regCost[0], grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a2a1f",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "Defining the gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09f39ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, num_iters, Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        #call CostFunctionReg \n",
    "        cost, grad = costFunctionReg(X, y, theta, Lambda)\n",
    "        \n",
    "        #update theta\n",
    "\n",
    "        theta = theta - np.dot(alpha,grad)\n",
    "        \n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22d3f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, y, initial_theta, alpha, num_iters, Lambda, K):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    numpy array of data X and labels y\n",
    "    initial_theta - inicialized vector of model parameters theta \n",
    "    alpha - learning rate\n",
    "    num_iters - number of iterations\n",
    "    Lambda - regularization parameter \n",
    "    K -number of classes\n",
    "    \n",
    "    ONEVSALL trains K Logistic Regression classifiers using gradient descent. \n",
    "    \n",
    "    Returns:   \n",
    "    all_theta - Kxn matrix where i-th row corresponds to the i-th classifier, n parameters\n",
    "    all_J - the evolution of cost function during each iteration (J_history) for all K classifiers\n",
    "    \n",
    "    \"\"\"\n",
    "    all_theta = []\n",
    "    all_J = []\n",
    "\n",
    "    #number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #number of features\n",
    "    n = X.shape[1]\n",
    "    \n",
    "    # add an extra column of 1Â´s corresponding to xo=1 (aka intercept term)\n",
    "    X = np.append(np.ones((m,1)),X,axis=1)\n",
    "    \n",
    "    for i in range(1,K+1):\n",
    "        theta , J_history = gradientDescent(X, np.where(y == i,1,0), initial_theta, alpha,num_iters, Lambda)\n",
    "        \n",
    "        # add the vector of optimized parameters theta of classifier i\n",
    "        all_theta.extend(theta)\n",
    "                \n",
    "        # add the cost function history of classifier i\n",
    "        all_J.extend(J_history)\n",
    "        \n",
    "    return np.array(all_theta).reshape(K,n+1), all_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebf90ab9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (67501,1) (2,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [79], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m num_iters \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[0;32m     15\u001b[0m Lambda \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[1;32m---> 17\u001b[0m all_theta, all_J \u001b[39m=\u001b[39m oneVsAll(data_used, label_used, initial_theta, alpha, num_iters, Lambda, K)\n",
      "Cell \u001b[1;32mIn [76], line 31\u001b[0m, in \u001b[0;36moneVsAll\u001b[1;34m(X, y, initial_theta, alpha, num_iters, Lambda, K)\u001b[0m\n\u001b[0;32m     28\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mones((m,\u001b[39m1\u001b[39m)),X,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,K\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m     theta , J_history \u001b[39m=\u001b[39m gradientDescent(X, np\u001b[39m.\u001b[39;49mwhere(y \u001b[39m==\u001b[39;49m i,\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m), initial_theta, alpha,num_iters, Lambda)\n\u001b[0;32m     33\u001b[0m     \u001b[39m# add the vector of optimized parameters theta of classifier i\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     all_theta\u001b[39m.\u001b[39mextend(theta)\n",
      "Cell \u001b[1;32mIn [75], line 18\u001b[0m, in \u001b[0;36mgradientDescent\u001b[1;34m(X, y, theta, alpha, num_iters, Lambda)\u001b[0m\n\u001b[0;32m     14\u001b[0m     cost, grad \u001b[39m=\u001b[39m costFunctionReg(X, y, theta, Lambda)\n\u001b[0;32m     16\u001b[0m     \u001b[39m#update theta\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     theta \u001b[39m=\u001b[39m theta \u001b[39m-\u001b[39;49m np\u001b[39m.\u001b[39;49mdot(alpha,grad)\n\u001b[0;32m     20\u001b[0m     J_history\u001b[39m.\u001b[39mappend(cost)\n\u001b[0;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m theta , J_history\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (67501,1) (2,1) "
     ]
    }
   ],
   "source": [
    "# defining K - number of classes\n",
    "K = len(os.listdir(cwd)) # number of unique Pokemon\n",
    "\n",
    "data_used = data_final[0:10]\n",
    "label_used = np.array(pokemon_labels[0:10])\n",
    "\n",
    "m = data_used.shape[0]\n",
    "n = data_used.shape[1]\n",
    "\n",
    "initial_theta = np.zeros((n+1,1))\n",
    "\n",
    "# Hyper-parameters\n",
    "alpha = 1 # learning rate\n",
    "num_iters = 300\n",
    "Lambda = 0.1\n",
    "\n",
    "all_theta, all_J = oneVsAll(data_used, label_used, initial_theta, alpha, num_iters, Lambda, K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21b2fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the gradient of sigmoid function\n",
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    computes the gradient of the sigmoid function\n",
    "    \"\"\"\n",
    "    sigmoid = sigmoid(z)\n",
    "    \n",
    "    return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7c29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda):\n",
    "    \"\"\"\n",
    "    nn_params contains the parameters unrolled into a vector\n",
    "    \n",
    "    compute the cost and gradient of the neural network\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2\n",
    "    Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "    Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "\n",
    "    J=0\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    y10 = np.zeros((m,num_labels))\n",
    "    \n",
    "    a1 = sigmoid(X @ Theta1.T)\n",
    "    a1 = np.hstack((np.ones((m, n, n,1)), a1)) # hidden layer\n",
    "    a2 = sigmoid(a1 @ Theta2.T) # output layer\n",
    "    \n",
    "    for i in range(1,num_labels+1):\n",
    "        y10[:,i-1][:,np.newaxis] = np.where(y==i,1,0)\n",
    "    for j in range(num_labels):\n",
    "        J = J + sum(-y10[:,j] * np.log(a2[:,j]) - (1-y10[:,j])*np.log(1-a2[:,j]))\n",
    "    \n",
    "    cost = 1/m* J\n",
    "    reg_J = cost + Lambda/(2*m) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))\n",
    "    \n",
    "    # Implement the backpropagation algorithm to compute the gradients\n",
    "    \n",
    "    grad1 = np.zeros((Theta1.shape))\n",
    "    grad2 = np.zeros((Theta2.shape))\n",
    "    \n",
    "    for i in range(m):\n",
    "        xi= X[i,:] # 1 X 401\n",
    "        a1i = a1[i,:] # 1 X 26\n",
    "        a2i =a2[i,:] # 1 X 10\n",
    "        d2 = a2i - y10[i,:]\n",
    "        d1 = Theta2.T @ d2.T * sigmoidGradient(np.hstack((1,xi @ Theta1.T)))\n",
    "        grad1= grad1 + d1[1:][:,np.newaxis] @ xi[:,np.newaxis].T\n",
    "        grad2 = grad2 + d2.T[:,np.newaxis] @ a1i[:,np.newaxis].T\n",
    "        \n",
    "    grad1 = 1/m * grad1\n",
    "    grad2 = 1/m*grad2\n",
    "    \n",
    "    grad1_reg = grad1 + (Lambda/m) * np.hstack((np.zeros((Theta1.shape[0],1)),Theta1[:,1:]))\n",
    "    grad2_reg = grad2 + (Lambda/m) * np.hstack((np.zeros((Theta2.shape[0],1)),Theta2[:,1:]))\n",
    "    \n",
    "    return cost, grad1, grad2, reg_J, grad1_reg, grad2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "38130d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    \"\"\"\n",
    "    randomly initializes the weights of a layer with L_in incoming connections and L_out outgoing connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    epi = (6**1/2) / (L_in + L_out)**1/2\n",
    "    \n",
    "    W = np.random.rand(L_out,L_in +1) *(2*epi) -epi\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "571b34b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = data_NN.shape[0]\n",
    "hidden_layer_size = 25\n",
    "\n",
    "num_labels = len(os.listdir(cwd))\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "initial_nn_params = np.append(initial_Theta1.flatten(),initial_Theta2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c6beb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentnn(X,y,initial_nn_params,alpha,num_iters,Lambda,input_layer_size, hidden_layer_size, num_labels):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    Theta1 = initial_nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "    Theta2 = initial_nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "    \n",
    "    m=len(y)\n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        nn_params = np.append(Theta1.flatten(),Theta2.flatten())\n",
    "        cost, grad1, grad2 = nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda)[3:]\n",
    "        Theta1 = Theta1 - (alpha * grad1)\n",
    "        Theta2 = Theta2 - (alpha * grad2)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    nn_paramsFinal = np.append(Theta1.flatten(),Theta2.flatten())\n",
    "    return nn_paramsFinal , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85e7b75c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 4 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [75], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m Lambda\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39m#Call gradientDescentnn\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m nnTheta, nnJ_history \u001b[39m=\u001b[39m gradientDescentnn(data_NN, pokemon_labels, initial_nn_params, alpha, num_iters, Lambda, input_layer_size, hidden_layer_size, num_labels)\n\u001b[0;32m     12\u001b[0m Theta1 \u001b[39m=\u001b[39m nnTheta[:((input_layer_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m hidden_layer_size)]\u001b[39m.\u001b[39mreshape(hidden_layer_size,input_layer_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m Theta2 \u001b[39m=\u001b[39m nnTheta[((input_layer_size \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m hidden_layer_size ):]\u001b[39m.\u001b[39mreshape(num_labels,hidden_layer_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [74], line 16\u001b[0m, in \u001b[0;36mgradientDescentnn\u001b[1;34m(X, y, initial_nn_params, alpha, num_iters, Lambda, input_layer_size, hidden_layer_size, num_labels)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iters):\n\u001b[0;32m     15\u001b[0m     nn_params \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(Theta1\u001b[39m.\u001b[39mflatten(),Theta2\u001b[39m.\u001b[39mflatten())\n\u001b[1;32m---> 16\u001b[0m     cost, grad1, grad2 \u001b[39m=\u001b[39m nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda)[\u001b[39m3\u001b[39m:]\n\u001b[0;32m     17\u001b[0m     Theta1 \u001b[39m=\u001b[39m Theta1 \u001b[39m-\u001b[39m (alpha \u001b[39m*\u001b[39m grad1)\n\u001b[0;32m     18\u001b[0m     Theta2 \u001b[39m=\u001b[39m Theta2 \u001b[39m-\u001b[39m (alpha \u001b[39m*\u001b[39m grad2)\n",
      "Cell \u001b[1;32mIn [71], line 16\u001b[0m, in \u001b[0;36mnnCostFunction\u001b[1;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda)\u001b[0m\n\u001b[0;32m     12\u001b[0m n \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m     15\u001b[0m J\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m---> 16\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhstack((np\u001b[39m.\u001b[39;49mones((m,\u001b[39m1\u001b[39;49m)),X))\n\u001b[0;32m     17\u001b[0m y10 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((m,num_labels))\n\u001b[0;32m     19\u001b[0m a1 \u001b[39m=\u001b[39m sigmoid(X \u001b[39m@\u001b[39m Theta1\u001b[39m.\u001b[39mT)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39mconcatenate(arrs, \u001b[39m0\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 4 dimension(s)"
     ]
    }
   ],
   "source": [
    "#It may take very long to finish the training. \n",
    "#For 1000 iterations the Training Set Accuracy: 95.28(lambda=0.1; alpha=1)\n",
    "#You may need more iterations (e.g. 1500) to get better accuracy\n",
    "\n",
    "alpha=3 #learning rate\n",
    "num_iters=1000\n",
    "Lambda=1\n",
    "\n",
    "#Call gradientDescentnn\n",
    "nnTheta, nnJ_history = gradientDescentnn(data_NN, pokemon_labels, initial_nn_params, alpha, num_iters, Lambda, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "Theta1 = nnTheta[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "Theta2 = nnTheta[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7f800a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e13dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "609973a2d1f31d45d1c4d9f5c0b4ecf9cb33fe1a555b03392724c0cdbb5c54ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
